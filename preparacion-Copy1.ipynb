{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Softmax\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import initializers\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backprop_helper import sigmoid, get_model, sigmoid_jac, softmax, softmax_jac, MSE, MSE_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward de la red\n",
    "Dados los pesos y la estructura de una red neuronal con una softmax a la salida y un MSE como Loss calcular todo lo que se pide a continuación (No es comun usar MSE con la softmax pero a fines didácticos simplifica. Queda como ejercicio adicional resolver el mismo ejercicio pero con una categorical crossentropy a la saluda)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "D1 (Dense)                   (None, 3)                 9         \n",
    "_________________________________________________________________\n",
    "A1 (Activation)              (None, 3)                 0         \n",
    "_________________________________________________________________\n",
    "D2 (Dense)                   (None, 2)                 8         \n",
    "_________________________________________________________________\n",
    "A2 (Activation)              (None, 2)                 0         \n",
    "_________________________________________________________________\n",
    "D3 (Dense)                   (None, 3)                 9         \n",
    "_________________________________________________________________\n",
    "P_est (Activation)           (None, 3)                 0         \n",
    "=================================================================\n",
    "Total params: 26\n",
    "Trainable params: 26\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capa Densa 1 ws: 2x3\n",
      "[[0.10820953 0.3432914  0.1744045 ]\n",
      " [0.05457611 0.54989725 0.34384015]]\n",
      "\n",
      "Capa Densa 1 biases:\n",
      "[-0.67943245 -0.00294854  0.15257952]\n",
      "\n",
      "Capa Densa 2 - ws: 3x3\n",
      "[[-0.7706185  -0.17550795]\n",
      " [-0.10197585  0.45046437]\n",
      " [ 0.00585397  0.3024927 ]]\n",
      "\n",
      "Capa Densa 2 - biases\n",
      "[-0.10661452 -0.34508756]\n",
      "\n",
      "Capa Densa 3 - ws: 3x3\n",
      "[[-0.49749678 -0.40208894 -0.85052264]\n",
      " [ 1.0619878   0.07141189  0.17314   ]]\n",
      "\n",
      "Capa Densa 3 - biases\n",
      "[-0.29359275 -0.7259881   0.578059  ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = np.load('weights_softmax_3_layers.npy', allow_pickle=True)\n",
    "capas = ['Capa Densa 1 ws: 2x3', 'Capa Densa 1 biases:', 'Capa Densa 2 - ws: 3x3', 'Capa Densa 2 - biases', 'Capa Densa 3 - ws: 3x3', 'Capa Densa 3 - biases']\n",
    "for i, layer in enumerate(weights):\n",
    "    print(capas[i])\n",
    "    print(layer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dimensión de entrada es 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37510012 0.14510518 0.4797947 ]]\n"
     ]
    }
   ],
   "source": [
    "# Vector de entrada de ejemplo\n",
    "X = np.array([[3.4, 2.1]])\n",
    "\n",
    "D1_out = X.dot(weights[0]) + weights[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1_out = sigmoid(D1_out)\n",
    "D2_out = A1_out.dot(weights[2]) + weights[3]\n",
    "A2_out = sigmoid(D2_out)\n",
    "\n",
    "D3_out = A2_out.dot(weights[4]) + weights[5]\n",
    "P_est = softmax(D3_out)\n",
    "print(P_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar la función MSE en numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.4 2.1]]\n",
      "[[1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X)\n",
    "# Vector de salida de ejemplo\n",
    "P_true = np.array([[1, 0, 0]])\n",
    "print(P_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,2) (1,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-961779091bee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/repos/Backpropagation/backprop_helper.py\u001b[0m in \u001b[0;36mMSE\u001b[0;34m(X_true, X_pred)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_true\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mX_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mMSE_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,2) (1,3) "
     ]
    }
   ],
   "source": [
    "MSE(X, P_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar la función softmax en numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78583498, 0.21416502]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector de ejemplo\n",
    "X = np.array([[3.4, 2.1]])\n",
    "\n",
    "softmax(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar una función que devuelva el jacobiano de la softmax evaluado en un vector columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector de ejemplo\n",
    "X = np.array([[3.4, 2.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16829836, -0.16829836],\n",
       "       [-0.16829836,  0.16829836]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_jac(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16829836, -0.16829836],\n",
       "       [-0.16829836,  0.16829836]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = softmax(X)\n",
    "np.diag(sm.reshape(-1)) - sm.reshape(-1, 1).dot(sm.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61753662, 0.16829836],\n",
       "       [0.16829836, 0.04586665]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.reshape(-1, 1).dot(sm.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16829836, -0.16829836],\n",
       "       [-0.16829836,  0.16829836]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_jac(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_model(lr = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "D1 (Dense)                   (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "A1 (Activation)              (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "D2 (Dense)                   (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "A2 (Activation)              (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "D3 (Dense)                   (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "P_est (Activation)           (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 26\n",
      "Trainable params: 26\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model('simple_model.hdf5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[3.4, 2.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37510014, 0.14510517, 0.4797947 ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37510012 0.14510518 0.4797947 ]]\n"
     ]
    }
   ],
   "source": [
    "D1_out = X.dot(weights[0]) + weights[1]\n",
    "A1_out = sigmoid(D1_out)\n",
    "D2_out = A1_out.dot(weights[2]) + weights[3]\n",
    "A2_out = sigmoid(D2_out)\n",
    "\n",
    "D3_out = A2_out.dot(weights[4]) + weights[5]\n",
    "P_est = softmax(D3_out)\n",
    "print(P_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si el y es igual a la salida entonces los pesos no se modifican\n",
    "P_true = np.array([[1, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37510012 0.14510518 0.4797947 ]]\n",
      "0.2139194440951749\n"
     ]
    }
   ],
   "source": [
    "loss = MSE(P_true, P_est)\n",
    "print(P_est)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 846us/step - loss: 0.2139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21391944587230682"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X, P_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05900569,  0.00456147,  0.05444422],\n",
       "       [-0.08949681,  0.0069186 ,  0.08257822]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backprop_D3_out = softmax_jac(D3_out).dot(MSE_grad(P_true, P_est).T)\n",
    "A2_out.T.dot(backprop_D3_out.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01295024, -0.00409727, -0.00559341],\n",
       "       [ 0.00799867, -0.00253067, -0.00345476]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backprop_D1_out = sigmoid_jac(D1_out).dot(weights[2].dot(sigmoid_jac(D2_out).dot(weights[4].dot(softmax_jac(D3_out).dot(MSE_grad(P_true, P_est).T)))))\n",
    "X.T.dot(backprop_D1_out.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.01295024 -0.00409727 -0.00559341]\n",
      " [ 0.00799868 -0.00253067 -0.00345476]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.constant(X)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    preds = model(inputs)\n",
    "    loss = model.loss(tf.constant(P_true), preds)\n",
    "\n",
    "grads = tape.gradient(loss, model.get_layer(\"D1\").trainable_variables)\n",
    "print(grads[0])\n",
    "# jacob = tape.jacobian(loss, model.get_layer(\"D3\").trainable_variables)\n",
    "# print(jacob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para D1\n",
    "$\\theta_t = \\theta_{t-1} - \\mathrm{learning\\_rate} * g_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2738fe1b8ae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0msigmoid_dif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD1_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid_dif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD2_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "(sigmoid_dif(D1_out).T*weights[2].dot(sigmoid_dif(D2_out).T*weights[4]*(-2*(y-y_)))).dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-646842468d9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid_dif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD1_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid_dif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD2_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "X*(sigmoid_dif(D1_out).T*weights[2].dot(sigmoid_dif(D2_out).T*weights[4]*(-2*(y-y_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivada del MSE evaluada en y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.48470968]]\n"
     ]
    }
   ],
   "source": [
    "# Derivada de la loss respecto a y_\n",
    "mse_der = -2*(y-y_)\n",
    "print(mse_der)\n",
    "prop_grad_y_ = mse_der"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta para D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.26015587 -0.94712252 -1.49670461]] [[-3.48470968]]\n"
     ]
    }
   ],
   "source": [
    "# Gradiente de y_ respecto a cada parámetro\n",
    "# Recordar que cuando derivo una capa dense respecto a cada parámetro, me da la entrada a esa cada. El Bias es como si entrarada con 1\n",
    "\n",
    "y__grad_ws_d3 = A2_out\n",
    "y__grad_bias_d3 = 1\n",
    "\n",
    "delta_ws_d3 = y__grad_ws_d3*prop_grad_y_*lr\n",
    "delta_bias_d3 = y__grad_bias_d3*prop_grad_y_*lr\n",
    "print(delta_ws_d3, delta_bias_d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.67218125e-08, 5.60790036e-08, 1.47877194e-07]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pesos utlima capa modificados\n",
    "new_weights[4].T - (weights[4].T - delta_ws_d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.2371895e-08]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bias ultima capa modificado\n",
    "new_weights[5].T - (weights[5].T - delta_bias_d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84538307]\n",
      " [1.28369178]\n",
      " [1.53521471]]\n",
      "[[0.19267999]\n",
      " [0.25407076]\n",
      " [0.37617463]]\n"
     ]
    }
   ],
   "source": [
    "# Con que entro a A2? D2_out\n",
    "# Necesito y__grad_in_d3 (Gradiente de y_ respecto a la entrada al bloque d3)\n",
    "y__grad_in_d3 = weights[4]\n",
    "prop_grad_d3 = y__grad_in_d3*prop_grad_y_\n",
    "print(prop_grad_d3) # Entrada de D3\n",
    "prop_grad_A2 = sigmoid_dif(D2_out).T*prop_grad_d3*lr\n",
    "print(prop_grad_A2) # Entrada de A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_ws_d2 = (prop_grad_A2*A1_out).T\n",
    "delta_bias_d2 = prop_grad_A2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.25856622e-09 -5.46572199e-09  1.02110236e-08]\n",
      " [-1.62406742e-08 -1.44461989e-08 -1.64342799e-08]]\n",
      "[[-1.48808340e-08  1.29551381e-09  1.10791643e-09]]\n"
     ]
    }
   ],
   "source": [
    "print(new_weights[2] - (weights[2] - delta_ws_d2))\n",
    "print(new_weights[3] - (weights[3] - delta_bias_d2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.19309713]\n",
      " [ 0.082746  ]]\n",
      "[[-0.04419043]\n",
      " [ 0.02066716]]\n"
     ]
    }
   ],
   "source": [
    "d2_out_grad_in_d2 = weights[2]\n",
    "prop_grad_d2 = d2_out_grad_in_d2.dot(prop_grad_A2)\n",
    "print(prop_grad_d2) \n",
    "prop_grad_A1 = sigmoid_dif(D1_out).T*prop_grad_d2*lr\n",
    "print(prop_grad_A1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04419043],\n",
       "       [ 0.02066716]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_grad_A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.15024746  0.07026835]\n",
      " [-0.0927999   0.04340104]]\n"
     ]
    }
   ],
   "source": [
    "delta_ws_d1 = (X*prop_grad_A1).T\n",
    "delta_bias_d1 = prop_grad_A1.T\n",
    "print(delta_ws_d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.94892052e-08 -8.69921603e-09]\n",
      " [-1.77648721e-08 -9.97487437e-09]]\n",
      "[[-2.40702032e-08 -4.74994019e-09]]\n"
     ]
    }
   ],
   "source": [
    "print(new_weights[0] - (weights[0] - delta_ws_d1))\n",
    "print(new_weights[1] - (weights[1] - delta_bias_d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "A3_model = Model(model.input, model.get_layer('A3').input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.19870196,  0.01208979, -0.80531573]], dtype=float32)>"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A3_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'D3_1/kernel:0' shape=(2, 3) dtype=float32, numpy=\n",
       " array([[ 0.3395361 , -0.58271265,  0.21878819],\n",
       "        [-0.0810146 , -0.3692061 , -0.07064136]], dtype=float32)>,\n",
       " <tf.Variable 'D3_1/bias:0' shape=(3,) dtype=float32, numpy=array([ 0.66752887, -0.21773213, -0.14207424], dtype=float32)>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(\"D3\").trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
      "array([[-0.01626888, -0.00131034,  0.00017072],\n",
      "       [-0.01004843, -0.00080932,  0.00010545]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-4.7849659e-03, -3.8539272e-04,  5.0213028e-05], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "model = load_model('simple_model.hdf5')\n",
    "inputs = tf.constant(X)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    preds = model(inputs)\n",
    "    loss = model.loss(tf.constant(P_true), preds)\n",
    "\n",
    "grads = tape.gradient(loss, model.get_layer(\"D1\").trainable_variables)\n",
    "print(grads)\n",
    "# grads = tape.gradient(loss, model.get_layer(\"D3\").trainable_variables)\n",
    "# print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.24122276e-02,  1.46758699e-03, -9.68974365e-05],\n",
       "       [ 7.66637585e-03,  9.06450788e-04, -5.98484167e-05]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(sigmoid_jac(D1_out).dot(weights[2].dot(sigmoid_jac(D2_out).dot(weights[4].dot(softmax_jac(D3_out).dot((-2*(P_est-P_true)).T))))).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00063273,  0.00029997,  0.00871013],\n",
       "       [-0.00039081,  0.00018528,  0.00537978]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(sigmoid_jac(D1_out).dot(weights[2].dot(sigmoid_jac(D2_out).dot(weights[4].dot(softmax_jac(D3_out).dot((-2*(P_est-P_true)).T))))).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "D1 (Dense)                   (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "A1 (Activation)              (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "D2 (Dense)                   (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "A2 (Activation)              (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "D3 (Dense)                   (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "A3 (Activation)              (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 26\n",
      "Trainable params: 26\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_63_4/kernel:0' shape=(2, 2) dtype=float32, numpy=\n",
       " array([[-0.0132913 ,  0.39277452],\n",
       "        [ 0.61490476, -0.11834475]], dtype=float32)>,\n",
       " <tf.Variable 'dense_63_4/bias:0' shape=(2,) dtype=float32, numpy=array([ 0.9016673 , -0.33314416], dtype=float32)>,\n",
       " <tf.Variable 'dense_64_4/kernel:0' shape=(2, 3) dtype=float32, numpy=\n",
       " array([[-0.4778273 ,  0.3336253 , -0.03871878],\n",
       "        [ 0.07612087,  0.27854922, -0.02718207]], dtype=float32)>,\n",
       " <tf.Variable 'dense_64_4/bias:0' shape=(3,) dtype=float32, numpy=array([ 0.36176467, -0.86578524, -0.10165475], dtype=float32)>,\n",
       " <tf.Variable 'dense_65_4/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
       " array([[0.47252566],\n",
       "        [0.73365825],\n",
       "        [0.7718087 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_65_4/bias:0' shape=(1,) dtype=float32, numpy=array([1.4085784], dtype=float32)>]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_65_4/Identity:0' shape=(None, 1) dtype=float32>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e6201d067514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   3847\u001b[0m   \"\"\"\n\u001b[1;32m   3848\u001b[0m   return gradients_module.gradients(\n\u001b[0;32m-> 3849\u001b[0;31m       loss, variables, colocate_gradients_with_ops=True)\n\u001b[0m\u001b[1;32m   3850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         unconnected_gradients)\n\u001b[0m\u001b[1;32m    173\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    489\u001b[0m   \u001b[0;34m\"\"\"Implementation of gradients().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m     raise RuntimeError(\"tf.gradients is not supported when eager execution \"\n\u001b[0m\u001b[1;32m    492\u001b[0m                        \"is enabled. Use tf.GradientTape instead.\")\n\u001b[1;32m    493\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msrc_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead."
     ]
    }
   ],
   "source": [
    "K.gradients(model.output, model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.GradientTape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
